{"cells":[{"cell_type":"markdown","metadata":{"id":"wtfTd7LjFUVB"},"source":["##*Surely someone has spent the time crafting the right model for the job.*\n","  "]},{"cell_type":"markdown","metadata":{"id":"YYCfAqmQHc3_"},"source":["#Transfer learning with tensorflow part 1: Feature extraction.\n","\n","Transfer learning is leveraging a working model's existing architecture and learned patterns for our own problem."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1324,"status":"ok","timestamp":1669117593236,"user":{"displayName":"Yonas Befirdu","userId":"00149218774581523940"},"user_tz":-180},"id":"cN3-LIzoE-7H","outputId":"66f81a33-3342-4ecb-c9ee-dceb55cedfef"},"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}],"source":["# Are we using a gpu\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"o-X29BggJBIU"},"source":["#1. Downloading and becoming one with the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2599,"status":"ok","timestamp":1669117596829,"user":{"displayName":"Yonas Befirdu","userId":"00149218774581523940"},"user_tz":-180},"id":"_JKKlNRFI9DH","outputId":"30938fa1-c690-416e-b7ef-6a1689732673"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-11-22 11:46:33--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.111.128, 142.251.167.128, 142.251.163.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.111.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168546183 (161M) [application/zip]\n","Saving to: ‘10_food_classes_10_percent.zip’\n","\n","10_food_classes_10_ 100%[===================>] 160.74M   230MB/s    in 0.7s    \n","\n","2022-11-22 11:46:34 (230 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n","\n"]}],"source":["#Get data (10% of our food class data)\n","import zipfile\n","#Download the data\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","zip_ref=zipfile.ZipFile('10_food_classes_10_percent.zip')\n","zip_ref.extractall()\n","zip_ref.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73,"status":"ok","timestamp":1669117596840,"user":{"displayName":"Yonas Befirdu","userId":"00149218774581523940"},"user_tz":-180},"id":"ccvPX3lMJpcJ","outputId":"7f6125c1-2a47-404e-d044-c22305da4d06"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 2 directories and 0 images in the 10_food_classes_10_percent\n","There are 10 directories and 0 images in the 10_food_classes_10_percent/test\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/sushi\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/chicken_curry\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/steak\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/ramen\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/pizza\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/chicken_wings\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/grilled_salmon\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/fried_rice\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/ice_cream\n","There are 0 directories and 250 images in the 10_food_classes_10_percent/test/hamburger\n","There are 10 directories and 0 images in the 10_food_classes_10_percent/train\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/sushi\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/chicken_curry\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/steak\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/ramen\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/pizza\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/chicken_wings\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/grilled_salmon\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/fried_rice\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/ice_cream\n","There are 0 directories and 75 images in the 10_food_classes_10_percent/train/hamburger\n"]}],"source":["#How many images in each folder\n","import os\n","#walk through ten percent directory and list the number of files\n","for dir_path,dir_names,file_names in os.walk('10_food_classes_10_percent'):\n"," print(f'There are {len(dir_names)} directories and {len(file_names)} images in the {dir_path}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcgH4u0eKuLw"},"outputs":[],"source":["#Each contain 75 images for training and 250 images for test as we take only the 10% of the previou training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tmPNM9eLADb"},"outputs":[],"source":["#But intuitively you might think using less data leads to less accuracy for deep  learning but transfer learning does the trick."]},{"cell_type":"markdown","metadata":{"id":"8eX3VhyPLcvE"},"source":["##Creating data loaders(preparing the data)\n"," \n","\n"," WE'll use the ImagaDataGenerator class to load in our images in batches."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3447,"status":"ok","timestamp":1669117600246,"user":{"displayName":"Yonas Befirdu","userId":"00149218774581523940"},"user_tz":-180},"id":"fWfd_74bLVnQ","outputId":"5e79279d-a77a-4b4b-8ce2-f76e3793677e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training_images:\n","Found 750 images belonging to 10 classes.\n","testing_images\n","Found 2500 images belonging to 10 classes.\n"]}],"source":["from keras.preprocessing.image import ImageDataGenerator\n","IMAGE_SHAPE=(224,224)\n","BATCH_SIZE=32\n","EPOCH=5\n","train_dir='10_food_classes_10_percent/train'\n","test_dir='10_food_classes_10_percent/test'\n","train_datagen=ImageDataGenerator(rescale=1./255)\n","test_datagen=ImageDataGenerator(rescale=1./255)\n","print('Training_images:')\n","train_data_10_percent=train_datagen.flow_from_directory(train_dir,batch_size=BATCH_SIZE,target_size=IMAGE_SHAPE,\n","                                                        class_mode='categorical' )\n","print('testing_images')\n","test_data= test_datagen.flow_from_directory(test_dir,batch_size=BATCH_SIZE,target_size=IMAGE_SHAPE,\n","                                            class_mode='categorical')\n"]},{"cell_type":"markdown","metadata":{"id":"kRRJzpgBOfug"},"source":["##Setting up callbacks\n","\n","Are thing to run whilst our model trains.\n","Callbacks are extra functionality you can add to  your models to be performed during or after training.\n","Some of the most popular ones are the following:\n","\n","* Tracking experiments with the tensorboard callback.\n","* Model checkpointing with the modelcheck point callback.\n","* Stopping a model from training callback."]},{"cell_type":"markdown","metadata":{"id":"j1l2w0atPuEt"},"source":["### Why do we need tensorboard callback\n","  Track them and compare the models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NX10KskOGz5"},"outputs":[],"source":["# Let's create a Tensorboard callback (functionized because we need to create a new one for each model)\n","import datetime\n","def create_tensorboard_callback(dir_name,experiment_name):\n","  log_dir=dir_name + '/'+ experiment_name+'/'+datetime.datetime.now().strftime('%Y%m%d-%H$M%S')\n","  tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","  print(f'saving tensorboard log files to :{log_dir}')\n","  return tensorboard_callback"]},{"cell_type":"markdown","metadata":{"id":"_M3eQPhQSnm1"},"source":["##Creating models using tensorflow hub\n","\n","In the past we've been using Tensorflow to create our own models layer by layer from **scratch!**\n","\n","Now we're going to do a similar process except that the majority of our layers are goig to come from Tensorflow Hub.\n","\n","We can access pre-trained models on:https://tfhub.dev/\n"]},{"cell_type":"markdown","metadata":{"id":"9kg84x9nVsHv"},"source":["Browsing the Tensorflow Hub page and sorting for image classification we have found the following feature model link: https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1"]},{"cell_type":"markdown","metadata":{"id":"cgrW-mS8WP0W"},"source":["# FEATURE EXTRACTION MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pa4xKjiDSfPm"},"outputs":[],"source":["# But let's compare the following 2 models\n","resnet_url= 'https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5'\n","efficientnet_url= 'https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7jSFKzdXUGq"},"outputs":[],"source":["# Import dependencies\n","# We're going to build the model but not from scratch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qX3stnAOX-SJ"},"outputs":[],"source":["# Let's make a  create model function to create a model form a URL\n","def create_model(model_url,num_classes):\n","  '''Takes a tfhub URL and creates a keras sequential moedl with it\n","  return a keras sequential model with model_url as feature extractor layer and \n","  DEnse output layer with the num_classes output neuron'''\n","\n","  #Download the pre_trained model and save it as a keras layer\n","  feature_extractor_layer=hub.KerasLayer(model_url,trainable=False,name='features_extraction_layer',\n","                                         input_shape=IMAGE_SHAPE+(3,)) # same as (224,224,3)\n","  #Freeze the already learned features.\n","  #Create our own mode\n","  model=tf.keras.Sequential([\n","      feature_extractor_layer,\n","      layers.Dense(num_classes,activation='softmax',name='output_layer')\n","  ])\n","  return model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B-W8H_W3aYxn"},"source":["## Creating and testing Resnet Tensorflow Hub feature extraction model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WPjd1rPanzM"},"outputs":[],"source":["#Create a Resnet model\n","resnet_model=create_model(resnet_url,10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1669117610883,"user":{"displayName":"Yonas Befirdu","userId":"00149218774581523940"},"user_tz":-180},"id":"QNPkutJCaYSI","outputId":"9db6a2ae-a032-45fa-fd7b-3a39409019c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," features_extraction_layer (  (None, 2048)             23564800  \n"," KerasLayer)                                                     \n","                                                                 \n"," output_layer (Dense)        (None, 10)                20490     \n","                                                                 \n","=================================================================\n","Total params: 23,585,290\n","Trainable params: 20,490\n","Non-trainable params: 23,564,800\n","_________________________________________________________________\n"]}],"source":["# Then compile our model\n","resnet_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kv1E40ZwcWt0"},"outputs":[],"source":["#AS we can see the 23,564,800 params are not trainable(the feature extraction model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WH-_J5nZciZD"},"outputs":[],"source":["#Compile our model\n","resnet_model.compile(loss='categorical_crossentropy',\n","                     optimizer=tf.keras.optimizers.Adam(),\n","                     metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PavBZWxcyr9","outputId":"12aaf03b-eaaf-4654-94e8-f9991e69d3c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["saving tensorboard log files to :tensorflow_hub/resnetv250/20221122-11$M50\n","Epoch 1/5\n","24/24 [==============================] - 445s 19s/step - loss: 1.8594 - accuracy: 0.3693 - val_loss: 1.1665 - val_accuracy: 0.6384\n","Epoch 2/5\n","24/24 [==============================] - 420s 18s/step - loss: 0.8588 - accuracy: 0.7453 - val_loss: 0.8524 - val_accuracy: 0.7220\n","Epoch 3/5\n","24/24 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.8480"]}],"source":["resnet_history= resnet_model.fit(train_data_10_percent,epochs=EPOCH,steps_per_epoch=len(train_data_10_percent),\n","                 validation_data=test_data,\n","                 validation_steps=len(test_data),callbacks=[create_tensorboard_callback('tensorflow_hub','resnetv250')])"]},{"cell_type":"markdown","metadata":{"id":"yIYaJEOvh93a"},"source":["90% training accurcy and close to 80% validation accuracy. \n","**TRAINED ONLY 10% OF THE DATA BUT GOT THIS MUCH ACCURACY UNLIKE PREVIOUS MODELS, *\"TRANSFER LEARNING\"* IS AMAZING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7tG_5DkfLIo"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","def plot_loss_curves(history):\n","  acc=history.history['accuracy']\n","  val_acc=history.history['val_accuracy']\n","  epochs=range(len(acc))\n","  plt.plot(epochs,acc,'r',label='training_accuracy')\n","  plt.plot(epochs,val_acc,'b',label=\"Validation_accuracy\")\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  loss=history.history['loss']\n","  val_loss=history.history['val_loss']\n","  plt.plot(epochs,loss,'b',label='Training_loss')\n","  plt.plot(epochs,val_loss,'r',label='validation_loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsNOA-Bgjh4D"},"outputs":[],"source":["plot_loss_curves(resnet_history)"]},{"cell_type":"markdown","metadata":{"id":"18XHPhJgkcPW"},"source":["###Creating and testing EfficientNetBo tennsorflow hub feature extraction model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63tuYcBRjl_y"},"outputs":[],"source":["#Let's build and compare it to the resnet model\n","efficientnet_model=create_model(efficientnet_url,num_classes=10)\n","efficientnet_model.compile(loss='categorical_crossentropy',\n","                           optimizer=tf.keras.optimizers.Adam(),\n","                           metrics=['accuracy'])\n","efficient_history=efficientnet_model.fit(train_data_10_percent,epochs=EPOCH,steps_per_epoch=len(train_data_10_percent),\n","                       validation_data=test_data,validation_steps=len(test_data),\n","                       callbacks=[create_tensorboard_callback('tensorflow_hub','efficientnetb0')])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0tP5Ku1mldf"},"outputs":[],"source":["plot_loss_curves(efficient_history)"]},{"cell_type":"code","source":["#How many layers does our effientnet b0 feature extractor have?\n","efficientnet_model.layers[0]\n","#The first layer has 309 weights\n","len(efficientnet_model.layers[0].weights)"],"metadata":{"id":"rLiUxI-wNKpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Different types of transfer learning\n","\n","* Feature extraction: Freeze all parameters of the transferred model.\n","* Fine tuning: Unfreeze some or all of the pretrained models(transferrred model).\n"," \n"],"metadata":{"id":"2kLoOdX6QzGM"}},{"cell_type":"markdown","source":["#Comparing our models using tensorboard.\n","\n","We don't need to scroll for looking the training logs to compare the model..especially if we tried number of models...We could use tensorboard to compare different logs.\n","\n","**NOTE:** When you upload things to tensorboard.dev, your experiments are public!"],"metadata":{"id":"iAHap0MxSRXf"}},{"cell_type":"code","source":["#Upload Tensorboard dev records\n","#!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n","# --name 'EfficientNetB0 vs. Resnet50V2' \\\n","# --description 'comparing two different TF hub feature extraction model architecturs using 10% of the training data'\n","# --one_shot"],"metadata":{"id":"aWvy5GcvSPpQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our tensorboard experiments are uploaded publically here: https://tensorboard.dev/experiment/TNwY68YRREK4jxzV20T7jA/"],"metadata":{"id":"Ou5mObPVVGmu"}},{"cell_type":"code","source":["#Check out what Tensorboard experiments you have\n","# !tensorboard dev list"],"metadata":{"id":"61lBFmPtUfPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#You can also delete an experiment we can using the experiment ID\n","# !tensorboard dev delete --experiment_id #ID NUMBER"],"metadata":{"id":"8WH0SmK1WyeZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#EXERCISES\n","\n","1. Build and fit model using the same data we have but with MobileNetV2 architecture.\n","2. Name 3 different image classification models on TensorFlow HUb that we haven't used.\n","3. Build a model to classifiy images of two different things you've taken photos of."],"metadata":{"id":"4b6I-yOqbAJu"}},{"cell_type":"markdown","source":["###1. Build and fit model using the MOblieNetV2 architecture."],"metadata":{"id":"VdDt6zKYbnwF"}},{"cell_type":"code","source":["#We have already defined the create model function so we just need the URL and num. of num_classes\n","#We have also created the callback tensorboard earlier.\n","mobilenet_url='https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5'\n","mobilenet_model=create_model(mobilenet_url,10)\n","mobilenet_model.compile(loss='categorical_crossentropy',\n","                        optimizer=tf.keras.optimizers.Adam(),\n","                        metrics=['accuracy'])\n","mobilenet_history=mobilenet_model.fit(train_data_10_percent,epochs=EPOCH,\n","                    steps_per_epoch=len(train_data_10_percent),\n","                    validation_data=test_data,validation_steps=len(test_data),\n","                    callbacks=[create_tensorboard_callback('tensorflowhub','mobilenetv2100224')])\n","\n","\n"],"metadata":{"id":"pu6J_GoHXChI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss_curves(mobilenet_history)"],"metadata":{"id":"NpkiCNN9fRlx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mobilenet_model.summary()\n"],"metadata":{"id":"lVOgbJrahNMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["efficientnet_model.summary()"],"metadata":{"id":"ZJB8wQwlhXhq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not so great as the efficient net probably because we have used same number of trainable parameters but the no. of feature learning non trainable parameter is clearly almostc half as the Efficient net."],"metadata":{"id":"EotXhI3DhH-p"}},{"cell_type":"markdown","source":["###2. Name 3 different image classification models other than what we just have used?\n","\n","*  imagenet/mobilenet_v1_100_128/quantops/classification\n","* edgetpu/vision/mobilenet-edgetpu-v2-feature-vector/m\n","* imagenet/mobilenet_v1_025_224/quantops/classification\n","\n","These are some of the many"],"metadata":{"id":"LvR4agetgBgx"}},{"cell_type":"markdown","source":["###3. Build a model to classify two images you have taken\n","\n","I have taken 10 pictures of two objects 'wuha coda' Amharic for water bottle...and another 'glue' for a glue stick ..Let's see if it can classifiy the water bootle and the glue...I am going to try it first using the efficient net.\n"],"metadata":{"id":"GAMc1-j6iELG"}},{"cell_type":"code","source":["!gdown --id 1f2kYd1omRUX5KmUUPwfeiVM2Rs-Ptg_n"],"metadata":{"id":"mfTdouIvg5sE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","zip_ref=zipfile.ZipFile('wehacoda_and_glue.zip')\n","zip_ref.extractall()\n","zip_ref.close()"],"metadata":{"id":"dQQsPtQJv9e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","for dir_path,dir_names,file_names in os.walk('wehacoda_and_glue'):\n","  print(f'There are {len(dir_names)} directories and {len(file_names)} images in {dir_path}')"],"metadata":{"id":"pSPHci3owck1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Then we can define the training and test datasets using ImageDataGenerators and directories\n","train_dir2='wehacoda_and_glue/train'\n","test_dir2='wehacoda_and_glue/test'\n","from keras.preprocessing.image import ImageDataGenerator\n","#create instances of the train and test datagens\n","train_datagen2= ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=50,\n","                                   height_shift_range=0.2,\n","                                   width_shift_range=0.2,\n","                                   shear_range=0.2,\n","                                   zoom_range=0.2,\n","                                   horizontal_flip=True,\n","                                   fill_mode='nearest')\n","test_datagen2=ImageDataGenerator(rescale=1./255)\n","train_data2=train_datagen2.flow_from_directory(train_dir2,batch_size=32,\n","                                               target_size=(224,224),\n","                                               class_mode='binary')\n","test_data2=test_datagen2.flow_from_directory(test_dir2,batch_size=32,\n","                                             target_size=(224,224),\n","                                             class_mode='binary')"],"metadata":{"id":"eRSiGJZuy3wf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pathlib\n","import numpy as np\n","data_dir=pathlib.Path('wehacoda_and_glue/train')\n","class_names=np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n","class_names"],"metadata":{"id":"OeItFQ6x9_Te"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Now let's create a new create model function because the previous one uses softmax but we want sigmoid for ours\n","def create_model2(model_url):\n","  feature_extractor_layer=hub.KerasLayer(model_url,trainable=False,\n","                                         name='feature_extraction_layer',\n","                                         input_shape=(224,224,3))\n","  model=tf.keras.Sequential([\n","      feature_extractor_layer,\n","      layers.Dense(1,activation='sigmoid',name='output_layer')\n","  ])\n","  return model"],"metadata":{"id":"8ZOyvzar3zNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["efficient_model2=create_model2(efficientnet_url)\n","efficient_model2.compile(loss='binary_crossentropy',\n","                         optimizer=tf.keras.optimizers.Adam(),\n","                         metrics=['accuracy'])\n","efficient2_history=efficient_model2.fit(train_data2,epochs=5,steps_per_epoch=len(train_data2),\n","                     validation_data=test_data2, validation_steps=len(test_data2))"],"metadata":{"id":"KdKRTRq_5aDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We need to plot the loss curves\n","plot_loss_curves(efficient2_history)"],"metadata":{"id":"qICwJ2lq7OUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can then assess the curves and then evaluate it using uploaded images\n","def load_and_prep_image(file_name,img_shape=224):\n","  img=tf.io.read_file(file_name)\n","  img=tf.image.decode_image(img)\n","  print('Initial shape:',img.shape)\n","  img=tf.image.resize(img,size=[img_shape,img_shape])\n","  img=img/255\n","  return img\n","def pred_and_plot(model,file_name,class_names=class_names):\n","  img=load_and_prep_image(file_name)\n","  pred=model.predict(tf.expand_dims(img,axis=0))\n","  pred_class=class_names[int(tf.round(pred))]\n","  plt.imshow(img)\n","  plt.title(f'Prediction:{pred_class}')\n","  plt.axis(False)"],"metadata":{"id":"_QPBurE_7_dW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown --id 16vaDS_6HgO6GujYQcvlLLnwuOemznmrK"],"metadata":{"id":"PV_5OnW-DUDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown --id 17ov1ulQS9m-eYajS47d846cWtMGYLXoQ"],"metadata":{"id":"7lzTpvtgDbvs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown --id 1IXIV0G2aSI6VDkir0ffuCWo9Qhg7jTYn"],"metadata":{"id":"1VJEes1dIO7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown --id 12JZp8h_EXN44BwEzt2aBPdHgWTDBRBoS"],"metadata":{"id":"TVosolQ5I6Js"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown --id 1ReNtMXmADqNmRhAgV46YIqUJ-oa8zfLn"],"metadata":{"id":"CjQypZtmI91L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown --id 1SC8hFRFeule9yjXqWzNaOgL0PMPnKhfb"],"metadata":{"id":"kcGDEQxLI-tE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7qktHEOoJoC3"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOKJY5LBwv164htItNOd0tZ"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}